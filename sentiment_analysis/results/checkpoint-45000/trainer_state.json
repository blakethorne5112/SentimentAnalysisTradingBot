{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 45000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 1.930603265762329,
      "learning_rate": 4.962962962962963e-05,
      "loss": 0.446,
      "step": 500
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 2.179600954055786,
      "learning_rate": 4.926e-05,
      "loss": 0.396,
      "step": 1000
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 1.8203277587890625,
      "learning_rate": 4.888962962962963e-05,
      "loss": 0.3806,
      "step": 1500
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 3.24397349357605,
      "learning_rate": 4.8519259259259264e-05,
      "loss": 0.3736,
      "step": 2000
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 2.1564972400665283,
      "learning_rate": 4.8148888888888894e-05,
      "loss": 0.3611,
      "step": 2500
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.855088949203491,
      "learning_rate": 4.777851851851852e-05,
      "loss": 0.3665,
      "step": 3000
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 2.424861431121826,
      "learning_rate": 4.740814814814815e-05,
      "loss": 0.3565,
      "step": 3500
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.9762729406356812,
      "learning_rate": 4.703777777777778e-05,
      "loss": 0.3485,
      "step": 4000
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.068711519241333,
      "learning_rate": 4.666740740740741e-05,
      "loss": 0.3554,
      "step": 4500
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.3643128871917725,
      "learning_rate": 4.629703703703704e-05,
      "loss": 0.3471,
      "step": 5000
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 2.058966875076294,
      "learning_rate": 4.592666666666667e-05,
      "loss": 0.3556,
      "step": 5500
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.080549716949463,
      "learning_rate": 4.55562962962963e-05,
      "loss": 0.3477,
      "step": 6000
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 2.494450807571411,
      "learning_rate": 4.518592592592593e-05,
      "loss": 0.3468,
      "step": 6500
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 1.7143117189407349,
      "learning_rate": 4.48162962962963e-05,
      "loss": 0.3432,
      "step": 7000
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.6511276960372925,
      "learning_rate": 4.444592592592593e-05,
      "loss": 0.3436,
      "step": 7500
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 2.795764923095703,
      "learning_rate": 4.407555555555556e-05,
      "loss": 0.3422,
      "step": 8000
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 2.2625536918640137,
      "learning_rate": 4.370518518518519e-05,
      "loss": 0.3381,
      "step": 8500
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.2798337936401367,
      "learning_rate": 4.333555555555556e-05,
      "loss": 0.338,
      "step": 9000
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 2.1599528789520264,
      "learning_rate": 4.2965185185185186e-05,
      "loss": 0.339,
      "step": 9500
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 2.1222612857818604,
      "learning_rate": 4.2594814814814815e-05,
      "loss": 0.3334,
      "step": 10000
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 2.31209397315979,
      "learning_rate": 4.2224444444444444e-05,
      "loss": 0.3376,
      "step": 10500
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 2.2698028087615967,
      "learning_rate": 4.185407407407407e-05,
      "loss": 0.3332,
      "step": 11000
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 2.1065685749053955,
      "learning_rate": 4.148370370370371e-05,
      "loss": 0.3356,
      "step": 11500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.5841742753982544,
      "learning_rate": 4.111407407407408e-05,
      "loss": 0.334,
      "step": 12000
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.992223858833313,
      "learning_rate": 4.0743703703703706e-05,
      "loss": 0.3288,
      "step": 12500
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 2.3859200477600098,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.3294,
      "step": 13000
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.5666102170944214,
      "learning_rate": 4.0002962962962964e-05,
      "loss": 0.3337,
      "step": 13500
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 2.1660563945770264,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.3343,
      "step": 14000
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 1.7232130765914917,
      "learning_rate": 3.926296296296296e-05,
      "loss": 0.3272,
      "step": 14500
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.460545778274536,
      "learning_rate": 3.88925925925926e-05,
      "loss": 0.3239,
      "step": 15000
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 1.5024666786193848,
      "learning_rate": 3.8522222222222226e-05,
      "loss": 0.3313,
      "step": 15500
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.6651052236557007,
      "learning_rate": 3.8152592592592594e-05,
      "loss": 0.3244,
      "step": 16000
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 2.2649953365325928,
      "learning_rate": 3.778222222222222e-05,
      "loss": 0.3238,
      "step": 16500
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 1.7864890098571777,
      "learning_rate": 3.741185185185185e-05,
      "loss": 0.3256,
      "step": 17000
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 1.8196848630905151,
      "learning_rate": 3.704148148148148e-05,
      "loss": 0.3211,
      "step": 17500
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.3337013721466064,
      "learning_rate": 3.667185185185185e-05,
      "loss": 0.3223,
      "step": 18000
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 1.7917277812957764,
      "learning_rate": 3.630148148148148e-05,
      "loss": 0.3208,
      "step": 18500
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 2.035700559616089,
      "learning_rate": 3.5931111111111115e-05,
      "loss": 0.3245,
      "step": 19000
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1.5823310613632202,
      "learning_rate": 3.5560740740740744e-05,
      "loss": 0.3261,
      "step": 19500
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.6751880645751953,
      "learning_rate": 3.519037037037037e-05,
      "loss": 0.3225,
      "step": 20000
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 2.4611268043518066,
      "learning_rate": 3.482074074074074e-05,
      "loss": 0.3237,
      "step": 20500
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.3783899545669556,
      "learning_rate": 3.445037037037037e-05,
      "loss": 0.3201,
      "step": 21000
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 1.6533749103546143,
      "learning_rate": 3.408e-05,
      "loss": 0.3206,
      "step": 21500
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.2716742753982544,
      "learning_rate": 3.370962962962963e-05,
      "loss": 0.3205,
      "step": 22000
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.6796464920043945,
      "learning_rate": 3.333925925925926e-05,
      "loss": 0.3167,
      "step": 22500
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.31390780210494995,
      "eval_runtime": 387.476,
      "eval_samples_per_second": 412.929,
      "eval_steps_per_second": 12.904,
      "step": 22500
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 2.0099003314971924,
      "learning_rate": 3.296962962962963e-05,
      "loss": 0.2684,
      "step": 23000
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 2.3103442192077637,
      "learning_rate": 3.259925925925926e-05,
      "loss": 0.2735,
      "step": 23500
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 2.0691146850585938,
      "learning_rate": 3.222888888888889e-05,
      "loss": 0.2722,
      "step": 24000
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 3.173093318939209,
      "learning_rate": 3.185851851851852e-05,
      "loss": 0.2758,
      "step": 24500
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 2.1257617473602295,
      "learning_rate": 3.148814814814815e-05,
      "loss": 0.2771,
      "step": 25000
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 2.314349889755249,
      "learning_rate": 3.1118518518518516e-05,
      "loss": 0.272,
      "step": 25500
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 2.369661331176758,
      "learning_rate": 3.0748148148148145e-05,
      "loss": 0.2758,
      "step": 26000
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 2.2834010124206543,
      "learning_rate": 3.0377777777777778e-05,
      "loss": 0.2771,
      "step": 26500
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.6639389991760254,
      "learning_rate": 3.0007407407407407e-05,
      "loss": 0.2729,
      "step": 27000
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 2.224512815475464,
      "learning_rate": 2.963777777777778e-05,
      "loss": 0.2762,
      "step": 27500
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 3.124877691268921,
      "learning_rate": 2.926740740740741e-05,
      "loss": 0.274,
      "step": 28000
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 2.1794471740722656,
      "learning_rate": 2.889703703703704e-05,
      "loss": 0.2726,
      "step": 28500
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 2.6039366722106934,
      "learning_rate": 2.852666666666667e-05,
      "loss": 0.2761,
      "step": 29000
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 2.3634941577911377,
      "learning_rate": 2.8157037037037037e-05,
      "loss": 0.2775,
      "step": 29500
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.1074159145355225,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.272,
      "step": 30000
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 2.255692720413208,
      "learning_rate": 2.7416296296296295e-05,
      "loss": 0.2757,
      "step": 30500
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 2.0929653644561768,
      "learning_rate": 2.7045925925925924e-05,
      "loss": 0.2735,
      "step": 31000
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.3158862590789795,
      "learning_rate": 2.667555555555556e-05,
      "loss": 0.2736,
      "step": 31500
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 1.5898133516311646,
      "learning_rate": 2.6305925925925928e-05,
      "loss": 0.2751,
      "step": 32000
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 1.9986395835876465,
      "learning_rate": 2.5935555555555557e-05,
      "loss": 0.2804,
      "step": 32500
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 2.8808202743530273,
      "learning_rate": 2.5565185185185186e-05,
      "loss": 0.2763,
      "step": 33000
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 2.1914961338043213,
      "learning_rate": 2.5194814814814816e-05,
      "loss": 0.2689,
      "step": 33500
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 2.5457844734191895,
      "learning_rate": 2.4824444444444445e-05,
      "loss": 0.2754,
      "step": 34000
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 1.930858850479126,
      "learning_rate": 2.4454074074074077e-05,
      "loss": 0.2771,
      "step": 34500
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 2.403399705886841,
      "learning_rate": 2.4084444444444445e-05,
      "loss": 0.2749,
      "step": 35000
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 2.2317676544189453,
      "learning_rate": 2.3714074074074074e-05,
      "loss": 0.2765,
      "step": 35500
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.0345399379730225,
      "learning_rate": 2.3343703703703703e-05,
      "loss": 0.2757,
      "step": 36000
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 2.194512128829956,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.273,
      "step": 36500
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 2.586815118789673,
      "learning_rate": 2.2603703703703704e-05,
      "loss": 0.2737,
      "step": 37000
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.8342350721359253,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.2713,
      "step": 37500
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 2.8923346996307373,
      "learning_rate": 2.1863703703703707e-05,
      "loss": 0.278,
      "step": 38000
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 3.2380483150482178,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.2678,
      "step": 38500
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 2.8403611183166504,
      "learning_rate": 2.1122962962962962e-05,
      "loss": 0.2737,
      "step": 39000
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 1.9030917882919312,
      "learning_rate": 2.075259259259259e-05,
      "loss": 0.2743,
      "step": 39500
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.749515414237976,
      "learning_rate": 2.038222222222222e-05,
      "loss": 0.2709,
      "step": 40000
    },
    {
      "epoch": 1.8,
      "grad_norm": 3.2277631759643555,
      "learning_rate": 2.0011851851851853e-05,
      "loss": 0.2769,
      "step": 40500
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 2.2747111320495605,
      "learning_rate": 1.9641481481481482e-05,
      "loss": 0.2731,
      "step": 41000
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 1.7364704608917236,
      "learning_rate": 1.927111111111111e-05,
      "loss": 0.2699,
      "step": 41500
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 2.808094024658203,
      "learning_rate": 1.890074074074074e-05,
      "loss": 0.2685,
      "step": 42000
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 2.4781880378723145,
      "learning_rate": 1.8531111111111112e-05,
      "loss": 0.2699,
      "step": 42500
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 2.803158760070801,
      "learning_rate": 1.8161481481481483e-05,
      "loss": 0.2718,
      "step": 43000
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 2.431698799133301,
      "learning_rate": 1.7791111111111112e-05,
      "loss": 0.2759,
      "step": 43500
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 1.7266756296157837,
      "learning_rate": 1.742074074074074e-05,
      "loss": 0.2682,
      "step": 44000
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 2.3685343265533447,
      "learning_rate": 1.7050370370370374e-05,
      "loss": 0.2648,
      "step": 44500
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.5575629472732544,
      "learning_rate": 1.668e-05,
      "loss": 0.2707,
      "step": 45000
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.3090008795261383,
      "eval_runtime": 408.877,
      "eval_samples_per_second": 391.316,
      "eval_steps_per_second": 12.229,
      "step": 45000
    }
  ],
  "logging_steps": 500,
  "max_steps": 67500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.8151291174912e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
