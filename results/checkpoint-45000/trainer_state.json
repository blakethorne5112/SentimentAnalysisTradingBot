{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 45000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 5.218725681304932,
      "learning_rate": 4.981481481481482e-05,
      "loss": 0.4721,
      "step": 500
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 4.176409721374512,
      "learning_rate": 4.962962962962963e-05,
      "loss": 0.4276,
      "step": 1000
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 6.959764003753662,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 0.404,
      "step": 1500
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 2.080993413925171,
      "learning_rate": 4.925925925925926e-05,
      "loss": 0.3964,
      "step": 2000
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 3.3020143508911133,
      "learning_rate": 4.9074074074074075e-05,
      "loss": 0.3914,
      "step": 2500
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.3111088275909424,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.3795,
      "step": 3000
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 4.274315357208252,
      "learning_rate": 4.8703703703703704e-05,
      "loss": 0.3793,
      "step": 3500
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 2.531586170196533,
      "learning_rate": 4.851851851851852e-05,
      "loss": 0.3706,
      "step": 4000
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.9184800386428833,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.3747,
      "step": 4500
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 2.025480031967163,
      "learning_rate": 4.814814814814815e-05,
      "loss": 0.3736,
      "step": 5000
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 3.6563169956207275,
      "learning_rate": 4.796296296296296e-05,
      "loss": 0.3699,
      "step": 5500
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.6562771797180176,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 0.3689,
      "step": 6000
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 2.697523593902588,
      "learning_rate": 4.759259259259259e-05,
      "loss": 0.3685,
      "step": 6500
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 2.3181252479553223,
      "learning_rate": 4.740740740740741e-05,
      "loss": 0.3687,
      "step": 7000
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.5720230340957642,
      "learning_rate": 4.722222222222222e-05,
      "loss": 0.3593,
      "step": 7500
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 2.4246208667755127,
      "learning_rate": 4.703703703703704e-05,
      "loss": 0.3617,
      "step": 8000
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 2.6419484615325928,
      "learning_rate": 4.685185185185185e-05,
      "loss": 0.3609,
      "step": 8500
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.716439723968506,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.3614,
      "step": 9000
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 2.956144094467163,
      "learning_rate": 4.648148148148148e-05,
      "loss": 0.356,
      "step": 9500
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 4.9061970710754395,
      "learning_rate": 4.62962962962963e-05,
      "loss": 0.3512,
      "step": 10000
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 3.4352242946624756,
      "learning_rate": 4.6111111111111115e-05,
      "loss": 0.3562,
      "step": 10500
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 1.794019103050232,
      "learning_rate": 4.592592592592593e-05,
      "loss": 0.3566,
      "step": 11000
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 2.456202983856201,
      "learning_rate": 4.5740740740740745e-05,
      "loss": 0.352,
      "step": 11500
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.8210859298706055,
      "learning_rate": 4.555555555555556e-05,
      "loss": 0.3504,
      "step": 12000
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 2.523944616317749,
      "learning_rate": 4.5370370370370374e-05,
      "loss": 0.3561,
      "step": 12500
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 3.0224688053131104,
      "learning_rate": 4.518518518518519e-05,
      "loss": 0.3521,
      "step": 13000
    },
    {
      "epoch": 0.3,
      "grad_norm": 2.1087915897369385,
      "learning_rate": 4.5e-05,
      "loss": 0.3618,
      "step": 13500
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 3.3642852306365967,
      "learning_rate": 4.481481481481482e-05,
      "loss": 0.35,
      "step": 14000
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 2.314297676086426,
      "learning_rate": 4.462962962962963e-05,
      "loss": 0.3468,
      "step": 14500
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 2.6570208072662354,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.3441,
      "step": 15000
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 2.92358660697937,
      "learning_rate": 4.425925925925926e-05,
      "loss": 0.345,
      "step": 15500
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 4.252293586730957,
      "learning_rate": 4.4074074074074076e-05,
      "loss": 0.3537,
      "step": 16000
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 1.8787747621536255,
      "learning_rate": 4.388888888888889e-05,
      "loss": 0.3446,
      "step": 16500
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 2.798572540283203,
      "learning_rate": 4.3703703703703705e-05,
      "loss": 0.3568,
      "step": 17000
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 3.2876524925231934,
      "learning_rate": 4.351851851851852e-05,
      "loss": 0.3441,
      "step": 17500
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.380894422531128,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.3505,
      "step": 18000
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 3.2207565307617188,
      "learning_rate": 4.314814814814815e-05,
      "loss": 0.343,
      "step": 18500
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 2.8580946922302246,
      "learning_rate": 4.296296296296296e-05,
      "loss": 0.3391,
      "step": 19000
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.9676648378372192,
      "learning_rate": 4.277777777777778e-05,
      "loss": 0.3373,
      "step": 19500
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 2.095691442489624,
      "learning_rate": 4.259259259259259e-05,
      "loss": 0.3486,
      "step": 20000
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 3.378355026245117,
      "learning_rate": 4.240740740740741e-05,
      "loss": 0.3444,
      "step": 20500
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 2.989161729812622,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.3536,
      "step": 21000
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 3.238535165786743,
      "learning_rate": 4.203703703703704e-05,
      "loss": 0.3391,
      "step": 21500
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 2.8887319564819336,
      "learning_rate": 4.185185185185185e-05,
      "loss": 0.3285,
      "step": 22000
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.8376977443695068,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.3475,
      "step": 22500
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 2.7534384727478027,
      "learning_rate": 4.148148148148148e-05,
      "loss": 0.3369,
      "step": 23000
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 1.826325535774231,
      "learning_rate": 4.12962962962963e-05,
      "loss": 0.3396,
      "step": 23500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.3919671773910522,
      "learning_rate": 4.111111111111111e-05,
      "loss": 0.3355,
      "step": 24000
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 2.9432950019836426,
      "learning_rate": 4.092592592592593e-05,
      "loss": 0.3453,
      "step": 24500
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 2.4687180519104004,
      "learning_rate": 4.074074074074074e-05,
      "loss": 0.3369,
      "step": 25000
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 1.7788968086242676,
      "learning_rate": 4.055555555555556e-05,
      "loss": 0.3442,
      "step": 25500
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 2.7149438858032227,
      "learning_rate": 4.0370370370370374e-05,
      "loss": 0.3304,
      "step": 26000
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 2.995229959487915,
      "learning_rate": 4.018518518518519e-05,
      "loss": 0.3291,
      "step": 26500
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.4752204418182373,
      "learning_rate": 4e-05,
      "loss": 0.3346,
      "step": 27000
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 2.1933224201202393,
      "learning_rate": 3.981481481481482e-05,
      "loss": 0.3311,
      "step": 27500
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 2.7699086666107178,
      "learning_rate": 3.962962962962963e-05,
      "loss": 0.3338,
      "step": 28000
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 2.532135486602783,
      "learning_rate": 3.944444444444445e-05,
      "loss": 0.3367,
      "step": 28500
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 2.3852145671844482,
      "learning_rate": 3.925925925925926e-05,
      "loss": 0.3394,
      "step": 29000
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 3.124450922012329,
      "learning_rate": 3.9074074074074076e-05,
      "loss": 0.3357,
      "step": 29500
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.1634480953216553,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.3355,
      "step": 30000
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 3.7954492568969727,
      "learning_rate": 3.8703703703703705e-05,
      "loss": 0.3318,
      "step": 30500
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 2.1114108562469482,
      "learning_rate": 3.851851851851852e-05,
      "loss": 0.3285,
      "step": 31000
    },
    {
      "epoch": 0.7,
      "grad_norm": 2.5036473274230957,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.332,
      "step": 31500
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.6320480108261108,
      "learning_rate": 3.814814814814815e-05,
      "loss": 0.3332,
      "step": 32000
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 4.83836030960083,
      "learning_rate": 3.7962962962962964e-05,
      "loss": 0.333,
      "step": 32500
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 2.8250532150268555,
      "learning_rate": 3.777777777777778e-05,
      "loss": 0.3344,
      "step": 33000
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 1.8195891380310059,
      "learning_rate": 3.759259259259259e-05,
      "loss": 0.3302,
      "step": 33500
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 2.2067275047302246,
      "learning_rate": 3.740740740740741e-05,
      "loss": 0.3265,
      "step": 34000
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 2.670703887939453,
      "learning_rate": 3.722222222222222e-05,
      "loss": 0.3309,
      "step": 34500
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 1.7395164966583252,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.333,
      "step": 35000
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 2.540688991546631,
      "learning_rate": 3.685185185185185e-05,
      "loss": 0.3351,
      "step": 35500
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.4299392700195312,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.3302,
      "step": 36000
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 2.6636738777160645,
      "learning_rate": 3.648148148148148e-05,
      "loss": 0.3206,
      "step": 36500
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 2.4247677326202393,
      "learning_rate": 3.62962962962963e-05,
      "loss": 0.3275,
      "step": 37000
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 2.296592950820923,
      "learning_rate": 3.611111111111111e-05,
      "loss": 0.3293,
      "step": 37500
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 3.25201678276062,
      "learning_rate": 3.592592592592593e-05,
      "loss": 0.3205,
      "step": 38000
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 2.4228103160858154,
      "learning_rate": 3.574074074074074e-05,
      "loss": 0.3252,
      "step": 38500
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 2.3469245433807373,
      "learning_rate": 3.555555555555556e-05,
      "loss": 0.3338,
      "step": 39000
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 3.6405293941497803,
      "learning_rate": 3.537037037037037e-05,
      "loss": 0.3243,
      "step": 39500
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.9123907089233398,
      "learning_rate": 3.518518518518519e-05,
      "loss": 0.3329,
      "step": 40000
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.230952262878418,
      "learning_rate": 3.5e-05,
      "loss": 0.3324,
      "step": 40500
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 3.2234692573547363,
      "learning_rate": 3.481481481481482e-05,
      "loss": 0.3369,
      "step": 41000
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 2.4221839904785156,
      "learning_rate": 3.4629629629629626e-05,
      "loss": 0.3262,
      "step": 41500
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.8720444440841675,
      "learning_rate": 3.444444444444445e-05,
      "loss": 0.3281,
      "step": 42000
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 2.6626639366149902,
      "learning_rate": 3.425925925925926e-05,
      "loss": 0.3186,
      "step": 42500
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 4.309190273284912,
      "learning_rate": 3.4074074074074077e-05,
      "loss": 0.3237,
      "step": 43000
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 2.530123710632324,
      "learning_rate": 3.388888888888889e-05,
      "loss": 0.3252,
      "step": 43500
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 3.236894369125366,
      "learning_rate": 3.3703703703703706e-05,
      "loss": 0.3301,
      "step": 44000
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 2.7043509483337402,
      "learning_rate": 3.351851851851852e-05,
      "loss": 0.312,
      "step": 44500
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.7563488483428955,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.3252,
      "step": 45000
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.32045111060142517,
      "eval_runtime": 4044.2591,
      "eval_samples_per_second": 39.562,
      "eval_steps_per_second": 2.473,
      "step": 45000
    }
  ],
  "logging_steps": 500,
  "max_steps": 135000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9075645587456e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
